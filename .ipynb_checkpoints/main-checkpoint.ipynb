{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boris Giba, 22.09.2019\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from ImbalancedDatasetSampler import ImbalancedDatasetSampler\n",
    "\n",
    "from Transformations import getTransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note paths\n",
    "directories={\n",
    "    \"train\": \"data/finalDataset/train\",\n",
    "    \"val\": \"data/finalDataset/val\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNet(object):\n",
    "    \"\"\"\n",
    "    Better-Recycling-Network:\n",
    "    Squeezenet-Network with added methods for training etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,numberOfOutputClasses,useFeatureExtraction,usePretrainedNetwork=True):\n",
    "        \n",
    "        self.numberOfOutputClasses=numberOfOutputClasses\n",
    "        self.useFeatureExtraction=useFeatureExtraction\n",
    "        self.usePretrainedNetwork=usePretrainedNetwork\n",
    "        \n",
    "        self.inputSize = 224\n",
    "        \n",
    "        self.model=self.initialiseModel()\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.model\n",
    "        \n",
    "    def initialiseModel(self):\n",
    "        \"\"\" \n",
    "        initialise squeezenet-architecture\n",
    "        \"\"\"\n",
    "        model = models.squeezenet1_1(pretrained=self.usePretrainedNetwork)\n",
    "        \n",
    "        self.freezeNetwork()\n",
    "        \n",
    "        model.classifier[1] = nn.Conv2d(512, self.numberOfOutputClasses, kernel_size=(1,1), stride=(1,1))\n",
    "        model.num_classes = self.numberOfOutputClasses\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def freezeNetwork(self):\n",
    "        if self.useFeatureExtraction:\n",
    "            for parameter in model.parameters():\n",
    "                parameter.requires_grad = False\n",
    "    \n",
    "    def getParametersToUpdate(self):\n",
    "        parametersToUpdate = self.model.parameters()\n",
    "        \n",
    "        if self.useFeatureExtraction or parametersToUpdate==[]:\n",
    "            parametersToUpdate = []\n",
    "            for name,parameter in self.model.named_parameters():\n",
    "                if parameter.requires_grad == True:\n",
    "                    parametersToUpdate.append(param)\n",
    "                    print(\"\\t\",name)\n",
    "                    \n",
    "        return parametersToUpdate\n",
    "    \n",
    "    def loadParameters(self,filename,device):\n",
    "        self.model.load_state_dict(torch.load(filename,map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(path,augmentation):\n",
    "    \"\"\"\n",
    "    returns dataset-object based on the images found in the given folder-path with certain, added augmentations\n",
    "    \n",
    "    inputs:\n",
    "    -path: str: path to dataset-folder (for more details \n",
    "        see https://pytorch.org/docs/stable/torchvision/datasets.html?highlight=imagefolder#torchvision.datasets.ImageFolder)\n",
    "    -augmentation: str (\"val\"/\"basic\"/\"augmented\"): declares augmentation strength\n",
    "        -val: only necessary transforms (ToTensor,Normalize,..)\n",
    "        -basic: same as val with the addition of RandomHorizontalFlip\n",
    "        -augmented: stronger transforms, including usage of the imgaug-module (see https://github.com/aleju/imgaug)\n",
    "            -includes random rotation, zoom, changing contrasts and/or colours, and more\n",
    "    \n",
    "    outputs:\n",
    "    -torchvision.datasets.folder.ImageFolder: dataset with the images contained in the given path\n",
    "    \"\"\"\n",
    "    \n",
    "    if augmentation==\"val\":\n",
    "        dataset = datasets.ImageFolder(path, transform=getTransforms(\"val\"))\n",
    "\n",
    "    elif augmentation==\"basic\":\n",
    "        dataset = datasets.ImageFolder(path, transform=getTransforms(\"basic\"))\n",
    "    \n",
    "    elif augmentation==\"augmented\":\n",
    "        dataset = datasets.ImageFolder(path, transform=getTransforms(\"augmentation\"))\n",
    "        \n",
    "    else:\n",
    "        datasetBasic = datasets.ImageFolder(path, transform=getTransforms(\"basic\"))\n",
    "        datasetAugmented = datasets.ImageFolder(path, transform=getTransforms(\"augmentation\"))\n",
    "        dataset=torch.utils.data.ConcatDataset((datasetBasic,datasetAugmented))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSampler(abbreviation,dataset):\n",
    "    \"\"\"\n",
    "    returns either an ImbalancedDatasetSampler (see https://github.com/ufoym/imbalanced-dataset-sampler)\n",
    "    or a WeightedRandomSampler for the given dataset\n",
    "    \n",
    "    inputs:\n",
    "    -abbreviation: str (\"WRS\"/\"IDS\"/...)\n",
    "    -dataset: object from class which is child of torch.utils.data.dataset\n",
    "    \n",
    "    output:\n",
    "    -torch.utils.data.WeightedRandomSampler/\n",
    "    -ImbalancedDatasetSampler: sampler for the given dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if abbreviation==\"WRS\": \n",
    "        #taken from https://discuss.pytorch.org/t/using-weightedrandomsampler-with-concatdataset/51968/2\n",
    "        \n",
    "        #Get all targets\n",
    "        targets = []\n",
    "        for _, target in dataset:\n",
    "            targets.append(target)\n",
    "        targets = torch.tensor(targets)\n",
    "        \n",
    "        # Compute samples weight (each sample should get its own weight)\n",
    "        class_sample_count = torch.tensor(\n",
    "            [(targets == t).sum() for t in torch.unique(targets, sorted=True)])\n",
    "        weight = 1. / class_sample_count.float()\n",
    "        samples_weight = torch.tensor([weight[t] for t in targets])\n",
    "        \n",
    "        sampler = utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        \n",
    "    else:\n",
    "        sampler=ImbalancedDatasetSampler(dataset)\n",
    "    \n",
    "    return sampler\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoaders(trainDataset,batchSize,sampler=None,numberOfWorkers=0):\n",
    "    \"\"\"\n",
    "    returns two dataloaders; one containing the training-data and containing the testing/validation-data\n",
    "    \n",
    "    inputs:\n",
    "    -trainDataset: dataset-object (e.g. ImageFolder): contains the training-data\n",
    "    -batchSize: int: determines the size of the data-batches\n",
    "    -sampler: sampler-object (e.g. WeightedRandomSampler): sampler, which shall be used during the training process\n",
    "    -numberOfWorkers: int: number of working units on the device (only relevant for memory-usage-optimisation)\n",
    "    \n",
    "    outputs:\n",
    "    -dict: contains two dataloaders, one with the given training dataset and one with a test/validation dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if sampler==None:\n",
    "        shuffle=True\n",
    "    else:\n",
    "        shuffle=False\n",
    "        \n",
    "    dataloaderDictionary={\n",
    "        \"train\": utils.data.DataLoader(trainDataset, batch_size=batchSize, sampler=sampler, shuffle=shuffle, num_workers=numberOfWorkers, pin_memory=True),\n",
    "        \"val\": utils.data.DataLoader(getDataset(directories[\"val\"],\"val\"), batch_size=batchSize, shuffle=True, num_workers=numberOfWorkers, pin_memory=True)}\n",
    "\n",
    "    return dataloaderDictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevice(forceCPU=False):\n",
    "    \"\"\"\n",
    "    returns device which shall be used for training\n",
    "    \n",
    "    inputs:\n",
    "    -forceCPU: boolean\n",
    "    \n",
    "    outputs:\n",
    "    -torch.device\n",
    "    \"\"\"\n",
    "    if forceCPU:\n",
    "        device=torch.device(\"cpu\")\n",
    "        \n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device=torch.device(\"cpu\")\n",
    "        \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOptimiser(name,net):\n",
    "    \"\"\"\n",
    "    -returns one of the following Optimisers:\n",
    "    Adam,RMSProp,SGD (only these were used in comparisons of the BRNet)\n",
    "    with the parameters of the given net\n",
    "    \n",
    "    inputs:\n",
    "    -name: str (\"RMSProp\"/\"SGD\"/...)\n",
    "    -net: BRNet\n",
    "    \n",
    "    outputs:\n",
    "    \n",
    "    -Optimizer: requested optimiser with the parameters of the given net\n",
    "    -str: name of the returned optimiser\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    if name==\"RMSprop\":\n",
    "        optimiser = optim.RMSprop(net.getParametersToUpdate(), lr=0.001)\n",
    "        optimiserName=\"RMSprop\"\n",
    "        \n",
    "    elif name==\"SGD\":\n",
    "        optimiser = optim.SGD(net.getParametersToUpdate(), lr=0.0005, momentum=0.9)\n",
    "        optimiserName=\"SGD\"\n",
    "    \n",
    "    else:\n",
    "        optimiser = optim.Adam(net.getParametersToUpdate(), lr=0.0001)\n",
    "        optimiserName=\"Adam\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    return (optimiser,optimiserName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, dataloaders, optimiser, optimiserName, device, criterion=nn.CrossEntropyLoss(), numEpochs=25,write=True,saveName=None):\n",
    "    \"\"\"\n",
    "    trains the given neural network on the given datasets using the given hyperparameters such as optimiser etc.\n",
    "    returns the model, as well as the history of the test-/validation-accuracy during the training process\n",
    "    \n",
    "    inputs:\n",
    "    -model: object of class, which is child of nn.Module (e.g. BRNet.model)\n",
    "    -dataloaders: dict of 2 dataloaders {\"train\":..., \"val\":...}\n",
    "    -optimiser: optimization algorithm ( example: optim.Adam(...) )\n",
    "    -optimiserName: str: name of the optimiser (needed for tracking the statistics)\n",
    "    -device: torch.device: device which shall be used for the training (CPU/GPU)\n",
    "    -criterion: loss-criterion ( example: nn.CrossEntropyLoss() )\n",
    "    -numEpochs: int: number of training epochs\n",
    "    -write: boolean: determines if statistics should be written to .txt-file\n",
    "    -saveName: str: name which shall be used for the save-file of the network; will not be saved if None\n",
    "    \n",
    "    outputs:\n",
    "    -object of class, which is child of nn.Module (e.g. BRNet.model): the passed neural network,\n",
    "        which has been trained with the given hyperparameters\n",
    "    -list: history of the test-/validation-accuracy during the training process of passed network\n",
    "    \"\"\"\n",
    "    phases=[\"train\", \"val\"]\n",
    "    \n",
    "    #transfer network to GPU (if available)\n",
    "    model = model.to(device)\n",
    "    #record current time\n",
    "    timeStart = time.time()\n",
    "    \n",
    "    #prepare variables for tracking accuracy and loss\n",
    "    trainAccHistory = []\n",
    "    valAccHistory = []\n",
    "    lossHistory=[]\n",
    "\n",
    "    highestTestAccModel = copy.deepcopy(model.state_dict())\n",
    "    highestTestAcc = 0.0\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, numEpochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        #each epoch has a training and validation phase\n",
    "        for phase in phases:\n",
    "            if phase == \"train\":\n",
    "                model.train()  #unfreeze network parameters\n",
    "            else:\n",
    "                model.eval()   #freeze network parameters\n",
    "\n",
    "            currentLoss = 0.0\n",
    "            currentCorrects = 0\n",
    "\n",
    "            #iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device) #transfer inputs to GPU (if available)\n",
    "                labels = labels.to(device) #transfer labels to GPU (if available)\n",
    "\n",
    "                #reset the gradient\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "                #feedforward\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    #get outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    #backward and optimize if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimiser.step()\n",
    "\n",
    "                #record current statistics\n",
    "                currentLoss += loss.item() * inputs.size(0)\n",
    "                currentCorrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            #record epoch statistics\n",
    "            epochLoss = currentLoss / len(dataloaders[phase].dataset)\n",
    "            epochAcc = currentCorrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epochLoss, epochAcc))\n",
    "\n",
    "            #deep-copy model with highest test-accuracy if in validation phase\n",
    "            if phase == \"val\" and epochAcc > highestTestAcc:\n",
    "                highestTestAcc = epochAcc\n",
    "                highestTestAccModel = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            if phase == \"val\":\n",
    "                valAccHistory.append(epochAcc.item())\n",
    "                \n",
    "            else:\n",
    "                trainAccHistory.append(epochAcc.item())\n",
    "            \n",
    "            lossHistory.append((epochLoss,phase))\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    #calculate elapsed time\n",
    "    timeElapsed = time.time() - timeStart\n",
    "    print(\"training complete in {:.0f}m {:.0f}s\".format(timeElapsed // 60, timeElapsed % 60))\n",
    "    print(\"best val Acc: {:4f}\".format(highestTestAcc))\n",
    "    \n",
    "    #write final statistics to file if wanted\n",
    "    if write:\n",
    "        batchSize=dataloaders[\"train\"].batch_size\n",
    "        for name,parameter in net.model.named_parameters():\n",
    "            if \"classifier\" in name:\n",
    "                useFeatureExtraction=True\n",
    "            else:\n",
    "                useFeatureExtraction=False\n",
    "            break\n",
    "        \n",
    "        accFile=open(\"accsData.txt\",\"a\")\n",
    "        accFile.write(\"\\n{0} {1} {2} {3} {4}\\n\".format(optimiserName,numEpochs,batchSize,useFeatureExtraction,highestTestAcc, \"/n\"))\n",
    "        accFile.close()\n",
    "\n",
    "        timeFile=open(\"timesData.txt\",\"a\")\n",
    "        timeFile.write(\"\\n{0} {1} {2} {3} {4}\\n\".format(optimiserName,numEpochs,batchSize,useFeatureExtraction,(timeElapsed // 60, timeElapsed % 60), \"/n\"))\n",
    "        timeFile.close()\n",
    "        \n",
    "        trainHistFile=open(\"trainHist.txt\",\"a\")\n",
    "        trainHistFile.write(\"\\n{0} {1} {2} {3} {4}\\n\".format(optimiserName,numEpochs,batchSize,useFeatureExtraction,trainAccHistory, \"/n\"))\n",
    "        trainHistFile.close()\n",
    "        \n",
    "        valHistFile=open(\"valHist.txt\",\"a\")\n",
    "        valHistFile.write(\"\\n{0} {1} {2} {3} {4}\\n\".format(optimiserName,numEpochs,batchSize,useFeatureExtraction,valAccHistory, \"/n\"))\n",
    "        valHistFile.close()\n",
    "        \n",
    "        lossHistFile=open(\"lossHist.txt\",\"a\")\n",
    "        lossHistFile.write(\"\\n{0} {1} {2} {3} {4}\\n\".format(optimiserName,numEpochs,batchSize,useFeatureExtraction,lossHistory, \"/n\"))\n",
    "        lossHistFile.close()\n",
    "        \n",
    "    if saveName!=None:\n",
    "        torch.save(model.state_dict(), name)\n",
    "        \n",
    "        \n",
    "\n",
    "    #load and return model with highest test-accuracy\n",
    "    model.load_state_dict(highestTestAccModel)\n",
    "    return model, valAccHistory\n",
    "\n",
    "def evaluateModel(model, dataloaders, optimiser, optimiserName, criterion=nn.CrossEntropyLoss(), numEpochs=1, phases=[\"val\"],write=False,saveName=None):\n",
    "    \"\"\"\n",
    "    same as trainModel, but without the training process -> only evaluation and only once (1 epoch)\n",
    "    returns only the history of the test/validation-accuracy\n",
    "    \"\"\"\n",
    "    model,valAccHistory=train_model(model, dataloaders, optimiser, optimiserName, criterion, numEpochs, phases, write, saveName)\n",
    "    return valAccHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassAccuracies(model,dataset,numberOfClasses=4):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy of the given model based on every single of the output classes and returns said accuracies\n",
    "    \n",
    "    inputs:\n",
    "    -model: object of class, which is child of nn.Module (e.g. BRNet.model): model, on which the evaluation shall be performed\n",
    "    -dataset:  dataset-object (e.g. ImageFolder): dataset containing the data on which the evaluation shall take place\n",
    "    -numberOfClasses: int: declares the number of output-classes of the given model\n",
    "    \n",
    "    outputs:\n",
    "    -list of tuple: each tuple contains one class name and the corresponding, calculated accuracy\n",
    "    \"\"\"\n",
    "    classes = (\"plastic\",\"metal\",\"paper\",\"glass\")\n",
    "    device=getDevice(forceCPU=True)\n",
    "    model = model.to(device)\n",
    "    batchSize=1\n",
    "    dataloader=utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    classCorrect = list(0. for i in range(numberOfClasses))\n",
    "    classTotal = list(0. for i in range(numberOfClasses))\n",
    "    classAccuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            inputs = inputs.to(device)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            \n",
    "            for i in range(batchSize): #loop through every entry of the batch\n",
    "                label = labels[i]\n",
    "\n",
    "                classCorrect[label] += c.item() #+0 or +1 depending on the correctness of the prediction \n",
    "\n",
    "                classTotal[label] += 1 #+1, because the overall amount increases\n",
    "    \n",
    "    for i in range(4):\n",
    "        classAccuracies.append((classes[i], 100 * classCorrect[i] / classTotal[i]))\n",
    "        \n",
    "    print(*classAccuracies)\n",
    "    \n",
    "    return classAccuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.9280 Acc: 0.6119\n",
      "val Loss: 0.6347 Acc: 0.7374\n",
      "\n",
      "training complete in 14m 25s\n",
      "best val Acc: 0.737430\n",
      "('plastic', 61.97718631178707) ('metal', 52.252252252252255) ('paper', 92.60969976905312) ('glass', 72.83582089552239)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    trainDataset=getDataset(directories[\"train\"],\"concat\")\n",
    "    dataLoaders=getDataLoaders(trainDataset,batchSize=8)\n",
    "    device=getDevice()\n",
    "    print(device)\n",
    "\n",
    "    net = BRNet(numberOfOutputClasses=4,useFeatureExtraction=False)\n",
    "    optimiser,optimiserName= getOptimiser(\"Adam\",net)\n",
    "\n",
    "    #train and evaluate\n",
    "    trainedModel,valAccHistory=trainModel(net(), dataLoaders, optimiser=optimiser, optimiserName=optimiserName,device=device, numEpochs=1)\n",
    "    net.model=trainedModel\n",
    "\n",
    "    valDataset=getDataset(directories[\"val\"],\"val\")\n",
    "    classAccuracies=getClassAccuracies(trainedModel,valDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('plastic', 79.08745247148289) ('metal', 59.45945945945946) ('paper', 86.14318706697459) ('glass', 70.44776119402985)\n"
     ]
    }
   ],
   "source": [
    "valDataset=getDataset(directories[\"val\"],\"val\")\n",
    "classAccuracies=getClassAccuracies(trainedModel,valDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('plastic', 79.08745247148289),\n",
       " ('metal', 59.45945945945946),\n",
       " ('paper', 86.14318706697459),\n",
       " ('glass', 70.44776119402985)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classAccuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,parameter in net.model.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        useFeatureExtraction=True\n",
    "    else:\n",
    "        useFeatureExtraction=False\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boris Giba, 22.09.2019\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import picamera\n",
    "import io\n",
    "import json\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from time import clock,sleep\n",
    "\n",
    "device=torch.device(\"cpu\") #Raspberry Pi does not provide a GPU\n",
    "\n",
    "with open(\"trashToName2.json\", \"r\") as f: #needed for assigning class-indices to class-names\n",
    "    trashToName = json.load(f)\n",
    "\n",
    "def initialiseModel(numberOfClasses=4, useFeatureExtraction=False, usePretrainedNetwork=True):\n",
    "    \"\"\" \n",
    "    squeezenet-architecture is initialised\n",
    "    \n",
    "    inputs:\n",
    "    -numberOfClasses: int: declares number of output classes of the network\n",
    "    -useFeatureExtraction: boolean: if True, everything but the classifier of the network will be frozen during training\n",
    "    -usePretrainedNetwork: boolean: if True, the network parameters will be pretrained\n",
    "    \n",
    "    outputs: tuple containing:\n",
    "    -object of class, which is child of nn.Module: neural network (squeezenet)\n",
    "    -int (here: 224): input-size of the model (of the first layer)\n",
    "    \"\"\"\n",
    "    model = models.squeezenet1_1(pretrained=use_pretrained)\n",
    "    #change last layer to fit the current number of output-classes\n",
    "    model.classifier[1] = nn.Conv2d(512, numberOfClasses, kernel_size=(1,1), stride=(1,1))\n",
    "    model.numberOfClasses = numberOfClasses\n",
    "    inputSize = 224\n",
    "\n",
    "    return (model, inputSize)\n",
    "\n",
    "#initialise model\n",
    "model, inputSize = initialiseModel(model_name, numberOfClasses, feature_extract, use_pretrained=True)\n",
    "\n",
    "#load trained network parameters\n",
    "model.load_state_dict(torch.load(\"BR-Network\",map_location=device))\n",
    "\n",
    "#define transformations\n",
    "dataTransforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.functional.rotate(),\n",
    "        transforms.CenterCrop(224)\n",
    "        transforms.RandomResizedCrop(inputSize),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(inputSize),\n",
    "        transforms.CenterCrop(inputSize),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "def takePicture():\n",
    "    \"\"\"\n",
    "    taken from: https://picamera.readthedocs.io/en/release-1.10/recipes1.html#capturing-to-a-pil-image\n",
    "    \n",
    "    captures a photo using the Raspberry-Pi-camera and returns it as a PIL-Image\n",
    "    \"\"\"\n",
    "    # Create the in-memory stream\n",
    "    stream = io.BytesIO()\n",
    "    with picamera.PiCamera() as camera:\n",
    "        camera.start_preview()\n",
    "        sleep(2)\n",
    "        camera.capture(stream, format=\"jpeg\")\n",
    "    # \"Rewind\" the stream to the beginning so we can read its content\n",
    "    stream.seek(0)\n",
    "    image = Image.open(stream)\n",
    "\n",
    "    #transformations are needed because the camera is fixed at a 90 degree angle and is not exactly directed towards the wall\n",
    "    image=transforms.functional.rotate(image,90)\n",
    "\n",
    "    image=transforms.functional.five_crop(image,675)[4]\n",
    "\n",
    "    image=transforms.functional.five_crop(image,625)[1]\n",
    "    \n",
    "    return image\n",
    "\n",
    "def predict(model=model,img=None):\n",
    "    \"\"\"\n",
    "    the given neural network is used to classify the given image; the prediction of said network,\n",
    "        as well as the used image are returned\n",
    "    \n",
    "    input:\n",
    "    -model: object of class, which is child of nn.Module (e.g. BRNet.model)\n",
    "    -img: PIL Image / None\n",
    "    \n",
    "    output: tuple containing:\n",
    "    -int: prediction of the network\n",
    "    -PIL Image: image which was used for the prediction (can be useful if no image was passed in the function call)\n",
    "    \"\"\"\n",
    "    global model\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    #check if image has been passed and if no, capture one\n",
    "    if img==None:\n",
    "        img = takePicture()\n",
    "        \n",
    "    #convert image to Tensor, then to Variable and pass it through the network, then print results\n",
    "    imgEvalTensor = data_transforms[\"val\"](img)\n",
    "    imgEvalTensor.unsqueeze_(0) #unsqueeze, because no batch is used here (batchSize=1)\n",
    "    data = Variable(imgEvalTensor)\n",
    "    out = model(data)\n",
    "    \n",
    "    #print prediction\n",
    "    print(trashToName[str(out.data.max(1,keepdim=True)[1].item())])\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return (out.data.max(1,keepdim=True)[1].item(),img)\n",
    "\n",
    "def predictWithTime(img=None):\n",
    "    \"\"\"\n",
    "    same as predict, but prints the elapsed time as well\n",
    "    \"\"\"\n",
    "    startTime=clock()\n",
    "    prediction,img=predict(img=img)\n",
    "    endTime=clock()\n",
    "    timeElapsed=endTime-startTime\n",
    "    print(timeElapsed)\n",
    "    return (prediction,img)\n",
    "\n",
    "def trainImage(img,labelInt,model=model):\n",
    "    \"\"\"\n",
    "    similar to predict, however the network is also trained on the image\n",
    "    \n",
    "    input:\n",
    "    -model: object of class, which is child of nn.Module (e.g. BRNet.model)\n",
    "    -img: PIL Image / None\n",
    "    -labelInt: int: describes the class of img (in this case: 0<=labelInt<=4)\n",
    "    \n",
    "    output:\n",
    "    -int: prediction of the network\n",
    "    \"\"\"\n",
    "    global model\n",
    "    #unfreeze model parameters for training\n",
    "    model.train()\n",
    "    \n",
    "    #convert image and label to Tensor, then to Variable\n",
    "    imgEvalTensor = data_transforms[\"val\"](img)\n",
    "    imgEvalTensor.unsqueeze_(0)\n",
    "    \n",
    "    labelTensor=torch.tensor(labelInt)\n",
    "    label=Variable(labelTensor)\n",
    "    label.unsqueeze_(0)\n",
    "    \n",
    "    inputData = Variable(imgEvalTensor)\n",
    "    \n",
    "    #define loss function and optimiser\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimiser = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    \n",
    "    #zero gradient, pass image through network and train network on image\n",
    "    optimiser.zero_grad()\n",
    "    out = model(input_data)\n",
    "    loss = criterion(out, label)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    #print prediction\n",
    "    print(trashToName[str(out.data.max(1,keepdim=True)[1].item())])\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return (model,out.data.max(1,keepdim=True)[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RPi.GPIO as GPIO\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "\n",
    "from NNPredict import predictWithTime,trainImage\n",
    "from distanzMessen3 import checkDistanceReductionLoop,checkDistanceIncrease,determineBaseDistance\n",
    "\n",
    "#set numbering mode of GPIO-pins to BCM\n",
    "GPIO.setmode(GPIO.BCM)\n",
    "\n",
    "#define important variables and pin numbers\n",
    "baseDistance = None\n",
    "\n",
    "lightPin1 = 4\n",
    "lightPin2 = 17\n",
    "lightPin3 = 27\n",
    "lightPin4 = 22\n",
    "\n",
    "lightPins=[lightPin1,lightPin2,lightPin3,lightPin4]\n",
    "\n",
    "lightPinRed = 6\n",
    "lightPinYellow = 25\n",
    "buttonPin = 5\n",
    "feedbackButtonPin1 = 12\n",
    "feedbackButtonPin2 = 16\n",
    "feedbackButtonPin3 = 20\n",
    "feedbackButtonPin4 = 21\n",
    "\n",
    "#setup GPIO-pins\n",
    "GPIO.setup(lightPin1, GPIO.OUT)\n",
    "GPIO.setup(lightPin2, GPIO.OUT)\n",
    "GPIO.setup(lightPin3, GPIO.OUT)\n",
    "GPIO.setup(lightPin4, GPIO.OUT)\n",
    "GPIO.setup(lightPinRed, GPIO.OUT)\n",
    "GPIO.setup(lightPinYellow, GPIO.OUT)\n",
    "GPIO.setup(buttonPin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "\n",
    "GPIO.setup(feedbackButtonPin1, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "GPIO.setup(feedbackButtonPin2, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "GPIO.setup(feedbackButtonPin3, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "GPIO.setup(feedbackButtonPin4, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "\n",
    "GPIO.output(lightPin1,GPIO.LOW)\n",
    "GPIO.output(lightPin2,GPIO.LOW)\n",
    "GPIO.output(lightPin3,GPIO.LOW)\n",
    "GPIO.output(lightPin4,GPIO.LOW)\n",
    "GPIO.output(lightPinRed,GPIO.LOW)\n",
    "GPIO.output(lightPinYellow,GPIO.HIGH)\n",
    "\n",
    "def determineLight(img=None):\n",
    "    \"\"\"\n",
    "    determines the pin-number of the LED which is to be powered to represent\n",
    "    the current classification prediction of the nerual network for the given image\n",
    "    \n",
    "    input:\n",
    "    -img: PIL Image / None\n",
    "    \n",
    "    output: tuple containing:\n",
    "    -int: pin-number of the LED which is to be powered\n",
    "    -PIL Image: image which was used for the prediction\n",
    "    \"\"\"\n",
    "    lightNumber,img=predictWithTime(img=img)\n",
    "    light=lightPins[lightNumber]\n",
    "    return (light,img)\n",
    "\n",
    "def clearLights():\n",
    "    \"\"\"\n",
    "    unpowers all LEDs\n",
    "    \"\"\"\n",
    "    GPIO.output(lightPin1,GPIO.LOW)\n",
    "    GPIO.output(lightPin2,GPIO.LOW)\n",
    "    GPIO.output(lightPin3,GPIO.LOW)\n",
    "    GPIO.output(lightPin4,GPIO.LOW)\n",
    "    GPIO.output(lightPinRed,GPIO.LOW)\n",
    "    GPIO.output(lightPinYellow,GPIO.LOW)\n",
    "    \n",
    "\n",
    "def expandNN(img,label):\n",
    "    \"\"\"\n",
    "    trains neural network on a given image + class label\n",
    "    \n",
    "    input:\n",
    "    -img: PIL Image\n",
    "    -label: int (here: 0<=label<=4)\n",
    "    \"\"\"\n",
    "    GPIO.output(light,GPIO.LOW)\n",
    "    GPIO.output(lightPinRed,GPIO.HIGH)\n",
    "    GPIO.output(lightPinYellow,GPIO.LOW)\n",
    "    \n",
    "    model,_=trainImage(img,label)\n",
    "    \n",
    "#mainloop\n",
    "try:\n",
    "    while True:\n",
    "        #check if distance to furthest object has decreased and update loop criterion\n",
    "        loopCriterion,currentButtonState=checkDistanceReductionLoop()\n",
    "        if loopCriterion:\n",
    "            sleep(0.5)\n",
    "            \n",
    "            #execute determineLight and set LEDs to inform user that the Pi is busy\n",
    "            GPIO.output(lightPinRed,GPIO.HIGH)\n",
    "            GPIO.output(lightPinYellow,GPIO.LOW)\n",
    "            light,img=determineLight()\n",
    "            GPIO.output(lightPinRed, GPIO.LOW)\n",
    "            GPIO.output(lightPinYellow,GPIO.HIGH)\n",
    "\n",
    "            #alreadyTrained shall track if the current image has already been used for training\n",
    "            alreadyTrained=0\n",
    "            \n",
    "            GPIO.output(light, GPIO.HIGH)\n",
    "            while loopCriterion: #check if user wants to perform a manual classification\n",
    "                \n",
    "                #perform manual classification if wanted\n",
    "                if alreadyTrained==0:\n",
    "                    if GPIO.input(feedbackButtonPin1)==GPIO.LOW:\n",
    "                        expandNN(img,0)\n",
    "                        alreadyTrained+=1\n",
    "                    elif GPIO.input(feedbackButtonPin2)==GPIO.LOW:\n",
    "                        expandNN(img,1)\n",
    "                        alreadyTrained+=1\n",
    "                    elif GPIO.input(feedbackButtonPin3)==GPIO.LOW:\n",
    "                        expandNN(img,2)\n",
    "                        alreadyTrained+=1\n",
    "                    elif GPIO.input(feedbackButtonPin4)==GPIO.LOW:\n",
    "                        expandNN(img,3)\n",
    "                        alreadyTrained+=1\n",
    "                        \n",
    "                #if manual classification has been performed, display new prediction\n",
    "                if alreadyTrained==1:\n",
    "                    light,img=determineLight(img=img)\n",
    "                    \n",
    "                    GPIO.output(lightPinRed, GPIO.LOW)\n",
    "                    GPIO.output(lightPinYellow,GPIO.HIGH)\n",
    "                    \n",
    "                    GPIO.output(light, GPIO.HIGH)\n",
    "                \n",
    "                if baseDistance==None:\n",
    "                    baseDistance=determineBaseDistance()\n",
    "                loopCriterion= not checkDistanceIncrease(baseDistance,currentButtonState) #check if object has been removed\n",
    "                sleep(0.2)\n",
    "                \n",
    "            GPIO.output(light,False)\n",
    "            \n",
    "        sleep(0.5)\n",
    "        \n",
    "finally: #save model, unpower the lights and cleanup\n",
    "    torch.save(model_ft.state_dict(), \"BR-Network\")\n",
    "    clearLights()\n",
    "    GPIO.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RPi.GPIO as GPIO\n",
    "import time\n",
    "from numpy import median\n",
    "\n",
    "#set numbering mode of GPIO-pins to BCM\n",
    "GPIO.setmode(GPIO.BCM)\n",
    "\n",
    "buttonPin=5\n",
    "triggerPin=26\n",
    "echoPin=18\n",
    "\n",
    "GPIO.setup(buttonPin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "GPIO.setup(triggerPin, GPIO.OUT)\n",
    "GPIO.setup(echoPin, GPIO.IN)\n",
    "\n",
    "def determineBaseDistance():\n",
    "    \"\"\"\n",
    "    determines distance of furthest object (should be solid wall or similar object),\n",
    "    by executing measureDistance 3 times and taking the median of the calculated distances\n",
    "    \n",
    "    output:\n",
    "    -distance to furthest object (i.e. wall) in cm: float\n",
    "    \"\"\"\n",
    "    distanceMedian1=measureDistanceMedian(10)\n",
    "    time.sleep(0.1)\n",
    "    distanceMedian2=measureDistanceMedian(10)\n",
    "    time.sleep(0.1)\n",
    "    distanceMedian3=measureDistanceMedian(10)\n",
    "    medianDistances=[distanceMedian1,distanceMedian2,distanceMedian3]\n",
    "    baseDistance=median(medianDistances)\n",
    "\n",
    "    print(baseDistance,\"baseDistance\")\n",
    "\n",
    "    return baseDistance\n",
    "    \n",
    "def measureDistance(baseDistance):\n",
    "    \"\"\"\n",
    "    -uses the ultrasonic sensor HC-SR04 to send out an ultrasonic burst\n",
    "        and tracks the time between sending out said burst and receiving it\n",
    "    -then calculates the distance by applying a slightly modified version of the formula distance=velocity*time\n",
    "    -if an error should occur (e.g. lost echo) the base distance is returned\n",
    "    \n",
    "    input:\n",
    "    -baseDistance: float\n",
    "    \n",
    "    output:\n",
    "    -current distance to furthest object in cm: float\n",
    "    \"\"\"\n",
    "    print(\"measuring..\")\n",
    "    GPIO.output(triggerPin, GPIO.HIGH)\n",
    "    time.sleep(0.001)\n",
    "    GPIO.output(triggerPin, GPIO.LOW)\n",
    "    breakCounter=0\n",
    "    breakCounter2=0\n",
    "\n",
    "    while GPIO.input(echoPin)==GPIO.LOW:\n",
    "        breakCounter+=1\n",
    "        if breakCounter==1000:\n",
    "            break\n",
    "    start=time.time()\n",
    "        \n",
    "    while GPIO.input(echoPin)==GPIO.HIGH:\n",
    "        breakCounter2+=1\n",
    "        if breakCounter2==1000:\n",
    "            break\n",
    "    end=time.time()\n",
    "\n",
    "    if not breakCounter==1000 and not breakCounter2==1000:\n",
    "\n",
    "        deltaTime=end-start\n",
    "\n",
    "        distance=deltaTime * 17000\n",
    "\n",
    "    else:\n",
    "        distance=baseDistance\n",
    "\n",
    "    return distance\n",
    "\n",
    "def measureDistanceMedian(baseDistance):\n",
    "    \"\"\"\n",
    "    measures distance to furthest object\n",
    "    by executing measureDistance 3 times and then taking the median of those values\n",
    "    \"\"\"\n",
    "    distance1=measureDistance(baseDistance)\n",
    "    time.sleep(0.1)\n",
    "    distance2=measureDistance(baseDistance)\n",
    "    time.sleep(0.1)\n",
    "    distance3=measureDistance(baseDistance)\n",
    "    distances=[distance1,distance2,distance3]\n",
    "    distanceMedian=median(distances)\n",
    "\n",
    "    print(distanceMedian,\"median\")\n",
    "\n",
    "    return distanceMedian\n",
    "\n",
    "def checkDistanceReduction(baseDistance,initialButtonState):\n",
    "    \"\"\"\n",
    "    -checks if the distance to the furthest object is reduced (meaning an object has been placed)\n",
    "        by tracking said distance multiple times and comparing the measurements\n",
    "    -if the button has been pressed to force a classification the sensor is made powerless for the current classification\n",
    "    \n",
    "    input:\n",
    "    -baseDistance: float\n",
    "    -initialButtonState: (GPIO.HIGH / GPIO.LOW) or (True / False) or (1 / 0)\n",
    "    \n",
    "    output:\n",
    "    -boolean: has a distance decrease been detected?\n",
    "    -see initialButtonState: current state of the button\n",
    "    \n",
    "    \"\"\"\n",
    "    newDistance=measureDistanceMedian(baseDistance)\n",
    "\n",
    "    currentButtonState=GPIO.input(buttonPin)\n",
    "    currentDifference=baseDistance-newDistance\n",
    "\n",
    "    if currentDifference>0.5 or initialButtonState!=currentButtonState:\n",
    "                \n",
    "        time.sleep(2)\n",
    "        newDistance=measureDistanceMedian(baseDistance)\n",
    "        newDifference=baseDistance-newDistance\n",
    "\n",
    "        if (newDifference>0.5 and newDifference<1000) or (\n",
    "        initialButtonState!=currentButtonState):\n",
    "                distanceDecrease=True\n",
    "                        \n",
    "        else:\n",
    "            distanceDecrease=False\n",
    "    else:\n",
    "        distanceDecrease=False\n",
    "\n",
    "    return (distanceDecrease,currentButtonState)\n",
    "\n",
    "def checkDistanceIncrease(baseDistance,initialButtonState):\n",
    "    \"\"\"\n",
    "    same as checkDistanceDecrease, but checks for an increase in distance rather than a reduction\n",
    "    \"\"\"\n",
    "    newDistance=measureDistanceMedian(baseDistance,initialButtonState)\n",
    "    \n",
    "    currentButtonState=GPIO.input(buttonPin)\n",
    "    currentDifference=baseDistance-newDistance\n",
    "\n",
    "    if currentDifference<-0.5 or initialButtonState!=currentButtonState:\n",
    "                \n",
    "        time.sleep(2)\n",
    "        newDistance=measureDistanceMedian(baseDistance)\n",
    "        newDifference=baseDistance-newDistance\n",
    "\n",
    "        if (newDifference<-0.5 and newDifference>-1000) or (\n",
    "        initialButtonState!=currentButtonState):\n",
    "                distanceIncrease=True\n",
    "                        \n",
    "        else:\n",
    "            distanceIncrease=False\n",
    "    else:\n",
    "        distanceIncrease=False\n",
    "\n",
    "    return distanceIncrease \n",
    "\n",
    "def checkDistanceReductionLoop():\n",
    "    \"\"\"\n",
    "    similar to checkDistanceReduction,\n",
    "    this however executes checkDistanceReduction continuously until a reduction has been detected\n",
    "    \n",
    "    output:\n",
    "    -same as checkDistanceReduction\n",
    "    \"\"\"\n",
    "        initialButtonState=GPIO.input(buttonPin)\n",
    "        baseDistance=determineBaseDistance()\n",
    "        distanceDecrease,currentButtonState=checkDistanceReduction(baseDistance,initialButtonState)\n",
    "        while not distanceDecrease:\n",
    "                distanceDecrease,currentButtonState=checkDistanceReduction(baseDistance,initialButtonState)\n",
    "                time.sleep(0.1)\n",
    "\n",
    "        return (distanceDecrease,currentButtonState)\n",
    "\n",
    "#final cleanup\n",
    "GPIO.cleanup()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
